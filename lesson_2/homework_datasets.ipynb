{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbb2aad4",
   "metadata": {},
   "source": [
    "# Задание 2: Работа с датасетами (30 баллов)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c9f5c7",
   "metadata": {},
   "source": [
    "Извиняюсь ,что не импортировал,у меня почему-то VS не видело файл из первой части"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "7fcfd263",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LinearModel(nn.Module):\n",
    "    def __init__(self,in_features,out_features):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(in_features,out_features)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.layer1(x)\n",
    "\n",
    "\n",
    "#l1_loss \n",
    "def l1_loss(output, target, activations, criterion, l1_lambda):\n",
    "    # Вычисляем основную потерю\n",
    "    main_loss = criterion(output, target)\n",
    "    \n",
    "    # Убедимся, что main_loss - скаляр\n",
    "    if main_loss.dim() > 0:\n",
    "        main_loss = main_loss.mean()  # Усредняем, если это не скаляр\n",
    "    \n",
    "    # Вычисляем L1-регуляризацию\n",
    "    l1_penalty = torch.norm(activations, p=1)\n",
    "    \n",
    "    # Убедимся, что l1_penalty - скаляр\n",
    "    if l1_penalty.dim() > 0:\n",
    "        l1_penalty = l1_penalty.mean()\n",
    "    \n",
    "    return main_loss + l1_lambda * l1_penalty\n",
    "\n",
    "\n",
    "#класс для ранней остановки\n",
    "class EarlyStopping:\n",
    "    \"\"\"\"\n",
    "    patience-количество эпох, которые нужно подождать, прежде чем остановиться, если улучшения не будет.\n",
    "    delta-Minimum change in the monitored quantity to qualify as an improvement.\n",
    "    best_score,best_model_state -Track the best validation score and model state.\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, patience=5, delta=0):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.counter = 0\n",
    "        self.best_model_state = None\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.best_model_state = model.state_dict()\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.best_model_state = model.state_dict()\n",
    "            self.counter = 0\n",
    "\n",
    "    def load_best_model(self, model):\n",
    "        model.load_state_dict(self.best_model_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "a4c5aa47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import os\n",
    "\n",
    "class ClassificationMetrics:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Инициализация класса метрик классификации\n",
    "        \"\"\"\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def to_one_hot(self, labels, num_classes):\n",
    "        \"\"\"\n",
    "        Преобразование меток в one-hot encoding\n",
    "        \n",
    "        Args:\n",
    "            labels: тензор меток (batch_size,)\n",
    "            num_classes: количество классов\n",
    "            \n",
    "        Returns:\n",
    "            one-hot тензор (batch_size, num_classes)\n",
    "        \"\"\"\n",
    "        return torch.nn.functional.one_hot(labels, num_classes).float()\n",
    "\n",
    "    def precision(self, y_true, y_pred, average='macro'):\n",
    "        \"\"\"\n",
    "        Вычисление метрики Precision\n",
    "        \n",
    "        Args:\n",
    "            y_true: истинные метки (batch_size,)\n",
    "            y_pred: предсказанные метки (batch_size,)\n",
    "            average: тип усреднения ('macro', 'micro', 'weighted')\n",
    "            \n",
    "        Returns:\n",
    "            precision score\n",
    "        \"\"\"\n",
    "        y_true = torch.tensor(y_true, dtype=torch.long).to(self.device)\n",
    "        y_pred = torch.tensor(y_pred, dtype=torch.long).to(self.device)\n",
    "        \n",
    "        num_classes = len(torch.unique(y_true))\n",
    "        \n",
    "        if average == 'micro':\n",
    "            true_positives = torch.sum(y_true == y_pred).float()\n",
    "            predicted_positives = len(y_pred)\n",
    "            return true_positives / predicted_positives if predicted_positives > 0 else 0\n",
    "        \n",
    "        precision_per_class = []\n",
    "        weights = []\n",
    "        \n",
    "        for c in range(num_classes):\n",
    "            true_positives = torch.sum((y_true == c) & (y_pred == c)).float()\n",
    "            predicted_positives = torch.sum(y_pred == c).float()\n",
    "            \n",
    "            precision = true_positives / predicted_positives if predicted_positives > 0 else 0\n",
    "            precision_per_class.append(precision)\n",
    "            \n",
    "            if average == 'weighted':\n",
    "                weights.append(torch.sum(y_true == c).float() / len(y_true))\n",
    "        \n",
    "        precision_per_class = torch.tensor(precision_per_class)\n",
    "        \n",
    "        if average == 'macro':\n",
    "            return torch.mean(precision_per_class).item()\n",
    "        elif average == 'weighted':\n",
    "            return torch.sum(precision_per_class * torch.tensor(weights)).item()\n",
    "        \n",
    "        return precision_per_class.tolist()\n",
    "\n",
    "    def recall(self, y_true, y_pred, average='macro'):\n",
    "        \"\"\"\n",
    "        Вычисление метрики Recall\n",
    "        \n",
    "        Args:\n",
    "            y_true: истинные метки (batch_size,)\n",
    "            y_pred: предсказанные метки (batch_size,)\n",
    "            average: тип усреднения ('macro', 'micro', 'weighted')\n",
    "            \n",
    "        Returns:\n",
    "            recall score\n",
    "        \"\"\"\n",
    "        y_true = torch.tensor(y_true, dtype=torch.long).to(self.device)\n",
    "        y_pred = torch.tensor(y_pred, dtype=torch.long).to(self.device)\n",
    "        \n",
    "        num_classes = len(torch.unique(y_true))\n",
    "        \n",
    "        if average == 'micro':\n",
    "            true_positives = torch.sum(y_true == y_pred).float()\n",
    "            actual_positives = len(y_true)\n",
    "            return true_positives / actual_positives if actual_positives > 0 else 0\n",
    "        \n",
    "        recall_per_class = []\n",
    "        weights = []\n",
    "        \n",
    "        for c in range(num_classes):\n",
    "            true_positives = torch.sum((y_true == c) & (y_pred == c)).float()\n",
    "            actual_positives = torch.sum(y_true == c).float()\n",
    "            \n",
    "            recall = true_positives / actual_positives if actual_positives > 0 else 0\n",
    "            recall_per_class.append(recall)\n",
    "            \n",
    "            if average == 'weighted':\n",
    "                weights.append(torch.sum(y_true == c).float() / len(y_true))\n",
    "        \n",
    "        recall_per_class = torch.tensor(recall_per_class)\n",
    "        \n",
    "        if average == 'macro':\n",
    "            return torch.mean(recall_per_class).item()\n",
    "        elif average == 'weighted':\n",
    "            return torch.sum(recall_per_class * torch.tensor(weights)).item()\n",
    "        \n",
    "        return recall_per_class.tolist()\n",
    "\n",
    "    def f1_score(self, y_true, y_pred, average='macro'):\n",
    "        \"\"\"\n",
    "        Вычисление метрики F1-score\n",
    "        \n",
    "        Args:\n",
    "            y_true: истинные метки (batch_size,)\n",
    "            y_pred: предсказанные метки (batch_size,)\n",
    "            average: тип усреднения ('macro', 'micro', 'weighted')\n",
    "            \n",
    "        Returns:\n",
    "            f1 score\n",
    "        \"\"\"\n",
    "        precision = self.precision(y_true, y_pred, average)\n",
    "        recall = self.recall(y_true, y_pred, average)\n",
    "        \n",
    "        if average in ['macro', 'weighted']:\n",
    "            return 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        elif average == 'micro':\n",
    "            return precision\n",
    "        \n",
    "        f1_scores = []\n",
    "        for p, r in zip(precision, recall):\n",
    "            f1 = 2 * (p * r) / (p + r) if (p + r) > 0 else 0\n",
    "            f1_scores.append(f1)\n",
    "        return f1_scores\n",
    "\n",
    "    def roc_auc(self, y_true, y_scores, multi_class='ovr'):\n",
    "        \"\"\"\n",
    "        Вычисление метрики ROC-AUC\n",
    "        \n",
    "        Args:\n",
    "            y_true: истинные метки (batch_size,)\n",
    "            y_scores: вероятности предсказаний (batch_size, num_classes)\n",
    "            multi_class: 'ovr' (one-vs-rest) или 'ovo' (one-vs-one)\n",
    "            \n",
    "        Returns:\n",
    "            roc-auc score\n",
    "        \"\"\"\n",
    "        y_true = torch.tensor(y_true, dtype=torch.long).cpu().numpy()\n",
    "        y_scores = torch.tensor(y_scores, dtype=torch.float).cpu().numpy()\n",
    "        \n",
    "        num_classes = y_scores.shape[1]\n",
    "        y_true_one_hot = self.to_one_hot(torch.tensor(y_true), num_classes).cpu().numpy()\n",
    "        \n",
    "        auc_scores = []\n",
    "        \n",
    "        if multi_class == 'ovr':\n",
    "            for i in range(num_classes):\n",
    "                fpr, tpr, _ = roc_curve(y_true_one_hot[:, i], y_scores[:, i])\n",
    "                auc_score = auc(fpr, tpr)\n",
    "                auc_scores.append(auc_score)\n",
    "            return np.mean(auc_scores)\n",
    "        \n",
    "        elif multi_class == 'ovo':\n",
    "            auc_sum = 0\n",
    "            n_pairs = 0\n",
    "            for i in range(num_classes):\n",
    "                for j in range(i+1, num_classes):\n",
    "                    mask = np.logical_or(y_true == i, y_true == j)\n",
    "                    y_true_binary = (y_true[mask] == i).astype(int)\n",
    "                    y_scores_binary = y_scores[mask, i] / (y_scores[mask, i] + y_scores[mask, j])\n",
    "                    fpr, tpr, _ = roc_curve(y_true_binary, y_scores_binary)\n",
    "                    auc_sum += auc(fpr, tpr)\n",
    "                    n_pairs += 1\n",
    "            return auc_sum / n_pairs if n_pairs > 0 else 0\n",
    "        \n",
    "        return auc_scores\n",
    "\n",
    "    def confusion_matrix(self, y_true, y_pred, plot=True, save_path=None):\n",
    "        \"\"\"\n",
    "        Вычисление и визуализация confusion matrix с возможностью сохранения\n",
    "        \n",
    "        Args:\n",
    "            y_true: истинные метки (batch_size,)\n",
    "            y_pred: предсказанные метки (batch_size,)\n",
    "            plot: флаг для отображения визуализации\n",
    "            save_path: путь для сохранения графика (например, 'confusion_matrix.png')\n",
    "            \n",
    "        Returns:\n",
    "            confusion matrix\n",
    "        \"\"\"\n",
    "        y_true = torch.tensor(y_true, dtype=torch.long).to(self.device)\n",
    "        y_pred = torch.tensor(y_pred, dtype=torch.long).to(self.device)\n",
    "        \n",
    "        num_classes = len(torch.unique(y_true))\n",
    "        cm = torch.zeros((num_classes, num_classes), dtype=torch.long)\n",
    "        \n",
    "        for t, p in zip(y_true, y_pred):\n",
    "            cm[t, p] += 1\n",
    "        \n",
    "        if plot:\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            sns.heatmap(cm.numpy(), annot=True, fmt='d', cmap='Blues')\n",
    "            plt.title('Confusion Matrix')\n",
    "            plt.ylabel('True Label')\n",
    "            plt.xlabel('Predicted Label')\n",
    "            \n",
    "            if save_path:\n",
    "                # Создаём директорию, если она не существует\n",
    "                os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "                plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "                print(f\"Confusion matrix сохранена в {save_path}\")\n",
    "            \n",
    "            plt.show()\n",
    "            plt.close()\n",
    "        \n",
    "        return cm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805d804f",
   "metadata": {},
   "source": [
    "Создайте кастомный класс датасета для работы с CSV файлами:\n",
    " - Загрузка данных из файла\n",
    " - Предобработка (нормализация, кодирование категорий)\n",
    " - Поддержка различных форматов данных (категориальные, числовые, бинарные и т.д.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e39420",
   "metadata": {},
   "source": [
    "2.2 Эксперименты с различными датасетами (15 баллов)\n",
    "<br>Найдите csv датасеты для регрессии и бинарной классификации и, применяя наработки из предыдущей части задания, обучите линейную и логистическую регрессию\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef1cfa6",
   "metadata": {},
   "source": [
    "Для регрессии возьмем датасет Prediction of Insurance Charges\n",
    "его колонки:\n",
    " -age\n",
    " -sex\n",
    " -bmi: индекс массы тела\n",
    " -Children: количество детей\n",
    " -Smoker: курящий/некурящий\n",
    " -Region: регион жизни\n",
    " -Charges(target):цена страховки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "0cf057e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, path_file, numeric_columns: list, string_columns: list, binary_columns: list, target_column: str):\n",
    "        self.path_file = path_file\n",
    "        self.numeric_columns = numeric_columns\n",
    "        self.string_columns = string_columns\n",
    "        self.binary_columns = binary_columns\n",
    "        self.target_column = target_column\n",
    "        self.label_encoders = {}\n",
    "        self.one_hot_encoders = {}\n",
    "\n",
    "        self.df = pd.read_csv(path_file)\n",
    "        self.df = self.df.dropna()\n",
    "\n",
    "        self._convert_numeric()\n",
    "        self._convert_string()\n",
    "        self._convert_binary()\n",
    "\n",
    "    def _convert_numeric(self):\n",
    "        if self.numeric_columns:\n",
    "            scaler = StandardScaler()\n",
    "            self.df[self.numeric_columns] = scaler.fit_transform(self.df[self.numeric_columns])\n",
    "\n",
    "    def _convert_string(self):\n",
    "        if self.string_columns:\n",
    "            for column in self.string_columns:\n",
    "                le = LabelEncoder()\n",
    "                self.df[column] = self.df[column].fillna('missing')\n",
    "                self.df[column] = le.fit_transform(self.df[column])\n",
    "                self.label_encoders[column] = le\n",
    "\n",
    "    def _convert_binary(self):\n",
    "        if self.binary_columns:\n",
    "            for column in self.binary_columns:\n",
    "                encoder = OneHotEncoder(sparse_output=False, drop='first')\n",
    "                transformed = encoder.fit_transform(self.df[[column]])\n",
    "                new_columns = [f\"{column}_{cat}\" for cat in encoder.categories_[0][1:]]\n",
    "                self.df[new_columns] = transformed\n",
    "                self.df = self.df.drop(column, axis=1)\n",
    "                self.one_hot_encoders[column] = encoder\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data_one_row = self.df.iloc[index]\n",
    "        train = data_one_row.drop(self.target_column).values.astype(np.float32)  # Преобразуем в numpy float32\n",
    "        target = np.array(data_one_row[self.target_column], dtype=np.float32)  # Преобразуем в numpy float32\n",
    "        return torch.tensor(train), torch.tensor(target)  # Возвращаем тензоры"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465f51d1",
   "metadata": {},
   "source": [
    "Обучение регрессии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "aaea316a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 1/(100,), Train Loss: 327476651.764706, Val Loss: 311880878.222222\n",
      "Epoch 2/(100,), Train Loss: 319082727.529412, Val Loss: 330084360.888889\n",
      "Epoch 3/(100,), Train Loss: 318165669.176471, Val Loss: 308756250.666667\n",
      "Epoch 4/(100,), Train Loss: 320297941.176471, Val Loss: 340686343.111111\n",
      "Epoch 5/(100,), Train Loss: 316716095.529412, Val Loss: 327233368.888889\n",
      "Epoch 6/(100,), Train Loss: 315532083.764706, Val Loss: 311472371.555556\n",
      "Epoch 7/(100,), Train Loss: 313731534.588235, Val Loss: 313265361.777778\n",
      "Epoch 8/(100,), Train Loss: 313639895.058824, Val Loss: 307650261.333333\n",
      "Epoch 9/(100,), Train Loss: 314554938.823529, Val Loss: 306559799.111111\n",
      "Epoch 10/(100,), Train Loss: 313533564.705882, Val Loss: 326352001.777778\n",
      "Epoch 11/(100,), Train Loss: 319079892.705882, Val Loss: 301189192.000000\n",
      "Epoch 12/(100,), Train Loss: 314771250.352941, Val Loss: 303986839.111111\n",
      "Epoch 13/(100,), Train Loss: 315511492.705882, Val Loss: 327465466.666667\n",
      "Epoch 14/(100,), Train Loss: 312416051.294118, Val Loss: 312948878.222222\n",
      "Epoch 15/(100,), Train Loss: 310811676.941176, Val Loss: 315533244.444444\n",
      "Epoch 16/(100,), Train Loss: 309135947.294118, Val Loss: 299452280.000000\n",
      "Epoch 17/(100,), Train Loss: 310998542.352941, Val Loss: 299018761.777778\n",
      "Epoch 18/(100,), Train Loss: 311686604.235294, Val Loss: 307610314.666667\n",
      "Epoch 19/(100,), Train Loss: 309884119.529412, Val Loss: 301447278.222222\n",
      "Epoch 20/(100,), Train Loss: 306668505.882353, Val Loss: 316739946.666667\n",
      "Epoch 21/(100,), Train Loss: 307095560.470588, Val Loss: 308905226.666667\n",
      "Epoch 22/(100,), Train Loss: 310485912.705882, Val Loss: 295621135.111111\n",
      "Epoch 23/(100,), Train Loss: 306173051.764706, Val Loss: 309563034.666667\n",
      "Epoch 24/(100,), Train Loss: 308551612.235294, Val Loss: 298361253.333333\n",
      "Epoch 25/(100,), Train Loss: 305316691.294118, Val Loss: 315149029.333333\n",
      "Epoch 26/(100,), Train Loss: 306256418.823529, Val Loss: 297072108.444444\n",
      "Epoch 27/(100,), Train Loss: 317498801.176471, Val Loss: 308157644.444444\n",
      "Epoch 28/(100,), Train Loss: 306012484.470588, Val Loss: 309655161.777778\n",
      "Epoch 29/(100,), Train Loss: 305502428.705882, Val Loss: 299952652.444444\n",
      "Epoch 30/(100,), Train Loss: 300992360.470588, Val Loss: 294257280.000000\n",
      "Epoch 31/(100,), Train Loss: 301415615.529412, Val Loss: 301045898.666667\n",
      "Epoch 32/(100,), Train Loss: 301106392.470588, Val Loss: 294193260.444444\n",
      "Epoch 33/(100,), Train Loss: 300510155.294118, Val Loss: 297890572.444444\n",
      "Epoch 34/(100,), Train Loss: 304449042.823529, Val Loss: 305114053.333333\n",
      "Epoch 35/(100,), Train Loss: 303025815.529412, Val Loss: 294975560.888889\n",
      "Epoch 36/(100,), Train Loss: 298569745.176471, Val Loss: 303545923.555556\n",
      "Epoch 37/(100,), Train Loss: 300022199.058824, Val Loss: 318049228.444444\n",
      "Epoch 38/(100,), Train Loss: 301002799.058824, Val Loss: 302364327.111111\n",
      "Epoch 39/(100,), Train Loss: 298723563.764706, Val Loss: 297444014.222222\n",
      "Epoch 40/(100,), Train Loss: 301023820.941176, Val Loss: 295704611.555556\n",
      "Epoch 41/(100,), Train Loss: 295585116.235294, Val Loss: 314177944.888889\n",
      "Epoch 42/(100,), Train Loss: 299482048.941176, Val Loss: 310486058.666667\n",
      "Epoch 43/(100,), Train Loss: 295375455.058824, Val Loss: 303131393.777778\n",
      "Epoch 44/(100,), Train Loss: 296439182.352941, Val Loss: 297926293.333333\n",
      "Epoch 45/(100,), Train Loss: 297639711.529412, Val Loss: 280722752.888889\n",
      "Epoch 46/(100,), Train Loss: 296627513.882353, Val Loss: 288071175.111111\n",
      "Epoch 47/(100,), Train Loss: 294583203.058824, Val Loss: 283368558.222222\n",
      "Epoch 48/(100,), Train Loss: 294889005.176471, Val Loss: 292303898.666667\n",
      "Epoch 49/(100,), Train Loss: 292356773.176471, Val Loss: 286300152.888889\n",
      "Epoch 50/(100,), Train Loss: 292281061.411765, Val Loss: 289885932.444444\n",
      "Epoch 51/(100,), Train Loss: 295614341.647059, Val Loss: 291771477.333333\n",
      "Epoch 52/(100,), Train Loss: 294320327.529412, Val Loss: 295863623.111111\n",
      "Epoch 53/(100,), Train Loss: 290315824.470588, Val Loss: 288666794.666667\n",
      "Epoch 54/(100,), Train Loss: 291812088.470588, Val Loss: 289161415.111111\n",
      "Epoch 55/(100,), Train Loss: 289817122.823529, Val Loss: 283898200.888889\n",
      "Epoch 56/(100,), Train Loss: 289767161.882353, Val Loss: 297143520.888889\n",
      "Epoch 57/(100,), Train Loss: 292142831.529412, Val Loss: 296113694.222222\n",
      "Epoch 58/(100,), Train Loss: 290669222.823529, Val Loss: 292990497.777778\n",
      "Epoch 59/(100,), Train Loss: 290061070.352941, Val Loss: 289446755.555556\n",
      "Epoch 60/(100,), Train Loss: 289466534.588235, Val Loss: 278903013.333333\n",
      "Epoch 61/(100,), Train Loss: 285426994.352941, Val Loss: 308799550.222222\n",
      "Epoch 62/(100,), Train Loss: 286025735.529412, Val Loss: 287278796.444444\n",
      "Epoch 63/(100,), Train Loss: 286899370.117647, Val Loss: 281471966.222222\n",
      "Epoch 64/(100,), Train Loss: 293329474.823529, Val Loss: 275617019.555556\n",
      "Epoch 65/(100,), Train Loss: 288479295.529412, Val Loss: 292534549.333333\n",
      "Epoch 66/(100,), Train Loss: 286800577.176471, Val Loss: 276142143.111111\n",
      "Epoch 67/(100,), Train Loss: 287349478.588235, Val Loss: 294712182.222222\n",
      "Epoch 68/(100,), Train Loss: 281093232.235294, Val Loss: 291273891.555556\n",
      "Epoch 69/(100,), Train Loss: 285200421.176471, Val Loss: 295407447.111111\n",
      "Epoch 70/(100,), Train Loss: 283166499.294118, Val Loss: 285679324.444444\n",
      "Epoch 71/(100,), Train Loss: 289219326.117647, Val Loss: 280367885.333333\n",
      "Epoch 72/(100,), Train Loss: 283827162.352941, Val Loss: 284841751.111111\n",
      "Epoch 73/(100,), Train Loss: 278751395.294118, Val Loss: 283408314.666667\n",
      "Epoch 74/(100,), Train Loss: 283599763.294118, Val Loss: 282461920.000000\n",
      "Epoch 75/(100,), Train Loss: 279321084.705882, Val Loss: 286253054.222222\n",
      "Epoch 76/(100,), Train Loss: 277033539.058824, Val Loss: 280045841.777778\n",
      "Epoch 77/(100,), Train Loss: 280989821.647059, Val Loss: 289805202.666667\n",
      "Epoch 78/(100,), Train Loss: 279467620.000000, Val Loss: 278482092.444444\n",
      "Epoch 79/(100,), Train Loss: 285228841.882353, Val Loss: 292241233.777778\n",
      "Epoch 80/(100,), Train Loss: 281688709.647059, Val Loss: 267410948.000000\n",
      "Epoch 81/(100,), Train Loss: 282499025.882353, Val Loss: 276948098.666667\n",
      "Epoch 82/(100,), Train Loss: 277033552.941176, Val Loss: 277734922.666667\n",
      "Epoch 83/(100,), Train Loss: 274224548.235294, Val Loss: 285767571.555556\n",
      "Epoch 84/(100,), Train Loss: 276074660.235294, Val Loss: 287959791.111111\n",
      "Epoch 85/(100,), Train Loss: 279881958.588235, Val Loss: 284077502.222222\n",
      "Epoch 86/(100,), Train Loss: 273818697.882353, Val Loss: 285496874.666667\n",
      "Epoch 87/(100,), Train Loss: 277821306.588235, Val Loss: 285279815.111111\n",
      "Epoch 88/(100,), Train Loss: 274134659.764706, Val Loss: 303169952.000000\n",
      "Epoch 89/(100,), Train Loss: 275122842.352941, Val Loss: 307641817.777778\n",
      "Epoch 90/(100,), Train Loss: 278417455.529412, Val Loss: 269641879.111111\n",
      "Epoch 91/(100,), Train Loss: 273903747.764706, Val Loss: 304949967.111111\n",
      "Epoch 92/(100,), Train Loss: 273755136.705882, Val Loss: 281378755.555556\n",
      "Epoch 93/(100,), Train Loss: 275040540.705882, Val Loss: 293410485.333333\n",
      "Epoch 94/(100,), Train Loss: 269294179.294118, Val Loss: 267008662.222222\n",
      "Epoch 95/(100,), Train Loss: 269669792.470588, Val Loss: 272840572.444444\n",
      "Epoch 96/(100,), Train Loss: 269446810.352941, Val Loss: 274926039.111111\n",
      "Epoch 97/(100,), Train Loss: 269309386.352941, Val Loss: 259330022.444444\n",
      "Epoch 98/(100,), Train Loss: 273905600.705882, Val Loss: 265208550.222222\n",
      "Epoch 99/(100,), Train Loss: 267286857.647059, Val Loss: 297177258.666667\n",
      "Epoch 100/(100,), Train Loss: 268682474.117647, Val Loss: 281712344.888889\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader,SubsetRandomSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch.optim as optim\n",
    "dataset = CustomDataset(path_file=\"data/insurance.csv\",numeric_columns=[\"age\",\"bmi\"],string_columns=[\"region\"],binary_columns=[\"sex\",\"smoker\"],target_column=\"charges\")\n",
    "\n",
    "\n",
    "\n",
    "batch_size: int = 32,\n",
    "num_epochs: int = 100,\n",
    "learning_rate: float = 0.001,\n",
    "l1_lambda: float = 0.01,\n",
    "patience: int = 5,\n",
    "validation_split: float = 0.2\n",
    "\n",
    "\n",
    "in_features = len(dataset.df.columns) - 1  # Все колонки, кроме целевой\n",
    "out_features = 1 \n",
    "\n",
    "\n",
    "model = LinearModel(in_features, out_features)\n",
    "criterion = nn.MSELoss()  # Функция потерь для регрессии\n",
    "optimizer = torch.optim.Adam(params =model.parameters(), lr=0.001)\n",
    "\n",
    "    # Разделяем данные на обучающую и валидационную выборки\n",
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "train_indices, val_indices = train_test_split(\n",
    "        indices, test_size=validation_split, random_state=42\n",
    "    )\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "val_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=32, sampler=train_sampler)\n",
    "val_loader = DataLoader(dataset, batch_size=32, sampler=val_sampler)\n",
    "\n",
    "    # Инициализируем раннюю остановку\n",
    "early_stopping = EarlyStopping(patience=patience, delta=0)\n",
    "\n",
    "    # Обучение\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(100):\n",
    "        # Обучающий режим\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to( dtype=torch.float32), target.to(dtype=torch.float32)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target.view(-1, 1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "\n",
    "        # Валидация\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for data, target in val_loader:\n",
    "                data, target = data.to( dtype=torch.float32), target.to( dtype=torch.float32)\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target.view(-1, 1))  # Только MSE для валидации\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.6f}, Val Loss: {avg_val_loss:.6f}\")\n",
    "\n",
    "       \n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60631e5",
   "metadata": {},
   "source": [
    "Датасет Титаник"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0255e800",
   "metadata": {},
   "source": [
    "<h3>Описание названия колонок:</h3>\n",
    "<ul>\n",
    "<li>survival - Выживание (0 = Нет; 1 = Да)</li>\n",
    "<li>pclass - Класс пассажира (1 = 1-й; 2 = 2-й; 3 = 3-й)</li>\n",
    "<li>name - Имя</li>\n",
    "<li>sex - Пол</li>\n",
    "<li>age - Возраст</li>\n",
    "<li>sibsp - Количество братьев/сестер или супругов на борту</li>\n",
    "<li>parch - Количество родителей/детей на борту</li>\n",
    "<li>ticket - Номер билета</li>\n",
    "<li>fare - Стоимость билета пассажира</li>\n",
    "<li>cabin - Каюта</li>\n",
    "<li>embarked - Порт посадки (C = Шербур; Q = Квинстаун; S = Саутгемптон)</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "20e451e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 1/(100,), Train Loss: 76.208521, Val Loss: 61.642212\n",
      "Epoch 2/(100,), Train Loss: 72.596861, Val Loss: 86.661995\n",
      "Epoch 3/(100,), Train Loss: 70.062453, Val Loss: 43.922116\n",
      "Epoch 4/(100,), Train Loss: 69.319249, Val Loss: 61.893475\n",
      "Epoch 5/(100,), Train Loss: 64.622758, Val Loss: 80.970697\n",
      "Epoch 6/(100,), Train Loss: 64.092899, Val Loss: 77.216120\n",
      "Epoch 7/(100,), Train Loss: 60.315874, Val Loss: 61.351910\n",
      "Epoch 8/(100,), Train Loss: 58.908494, Val Loss: 44.223593\n",
      "Epoch 9/(100,), Train Loss: 55.788217, Val Loss: 68.497637\n",
      "Epoch 10/(100,), Train Loss: 52.496656, Val Loss: 60.455069\n",
      "Epoch 11/(100,), Train Loss: 49.609174, Val Loss: 68.109592\n",
      "Epoch 12/(100,), Train Loss: 49.171133, Val Loss: 52.961517\n",
      "Epoch 13/(100,), Train Loss: 47.913427, Val Loss: 45.494944\n",
      "Epoch 14/(100,), Train Loss: 44.276793, Val Loss: 36.695910\n",
      "Epoch 15/(100,), Train Loss: 42.724540, Val Loss: 31.425717\n",
      "Epoch 16/(100,), Train Loss: 43.076008, Val Loss: 42.778643\n",
      "Epoch 17/(100,), Train Loss: 41.356461, Val Loss: 43.063581\n",
      "Epoch 18/(100,), Train Loss: 38.149782, Val Loss: 41.120481\n",
      "Epoch 19/(100,), Train Loss: 35.540269, Val Loss: 32.136312\n",
      "Epoch 20/(100,), Train Loss: 33.681372, Val Loss: 31.962292\n",
      "Epoch 21/(100,), Train Loss: 31.588335, Val Loss: 27.280349\n",
      "Epoch 22/(100,), Train Loss: 30.065899, Val Loss: 32.325939\n",
      "Epoch 23/(100,), Train Loss: 29.046578, Val Loss: 21.553588\n",
      "Epoch 24/(100,), Train Loss: 26.478189, Val Loss: 28.196826\n",
      "Epoch 25/(100,), Train Loss: 26.063814, Val Loss: 24.493015\n",
      "Epoch 26/(100,), Train Loss: 25.892245, Val Loss: 21.037297\n",
      "Epoch 27/(100,), Train Loss: 24.051103, Val Loss: 26.116362\n",
      "Epoch 28/(100,), Train Loss: 21.543825, Val Loss: 26.594450\n",
      "Epoch 29/(100,), Train Loss: 21.110867, Val Loss: 19.126502\n",
      "Epoch 30/(100,), Train Loss: 20.920940, Val Loss: 17.673387\n",
      "Epoch 31/(100,), Train Loss: 20.112951, Val Loss: 17.898199\n",
      "Epoch 32/(100,), Train Loss: 18.652364, Val Loss: 21.786302\n",
      "Epoch 33/(100,), Train Loss: 17.527496, Val Loss: 16.417271\n",
      "Epoch 34/(100,), Train Loss: 18.525825, Val Loss: 21.897326\n",
      "Epoch 35/(100,), Train Loss: 17.208311, Val Loss: 21.572118\n",
      "Epoch 36/(100,), Train Loss: 16.145898, Val Loss: 12.368608\n",
      "Epoch 37/(100,), Train Loss: 16.439232, Val Loss: 14.293675\n",
      "Epoch 38/(100,), Train Loss: 15.369490, Val Loss: 15.218122\n",
      "Epoch 39/(100,), Train Loss: 14.987104, Val Loss: 18.192430\n",
      "Epoch 40/(100,), Train Loss: 14.740720, Val Loss: 15.240154\n",
      "Epoch 41/(100,), Train Loss: 14.691065, Val Loss: 11.122233\n",
      "Epoch 42/(100,), Train Loss: 13.746778, Val Loss: 18.248538\n",
      "Epoch 43/(100,), Train Loss: 13.884185, Val Loss: 13.373236\n",
      "Epoch 44/(100,), Train Loss: 13.861207, Val Loss: 12.878723\n",
      "Epoch 45/(100,), Train Loss: 14.227794, Val Loss: 11.478629\n",
      "Epoch 46/(100,), Train Loss: 13.855150, Val Loss: 17.209638\n",
      "Epoch 47/(100,), Train Loss: 13.863811, Val Loss: 17.864932\n",
      "Epoch 48/(100,), Train Loss: 13.418147, Val Loss: 17.312092\n",
      "Epoch 49/(100,), Train Loss: 12.958056, Val Loss: 12.773031\n",
      "Epoch 50/(100,), Train Loss: 13.190932, Val Loss: 17.825966\n",
      "Epoch 51/(100,), Train Loss: 12.573167, Val Loss: 15.433047\n",
      "Epoch 52/(100,), Train Loss: 12.258716, Val Loss: 13.968217\n",
      "Epoch 53/(100,), Train Loss: 12.448993, Val Loss: 10.588022\n",
      "Epoch 54/(100,), Train Loss: 12.926164, Val Loss: 15.644982\n",
      "Epoch 55/(100,), Train Loss: 12.286175, Val Loss: 13.582280\n",
      "Epoch 56/(100,), Train Loss: 11.816014, Val Loss: 14.633516\n",
      "Epoch 57/(100,), Train Loss: 12.181521, Val Loss: 11.764385\n",
      "Epoch 58/(100,), Train Loss: 11.316237, Val Loss: 16.113996\n",
      "Epoch 59/(100,), Train Loss: 11.620144, Val Loss: 18.460847\n",
      "Epoch 60/(100,), Train Loss: 11.808179, Val Loss: 12.785736\n",
      "Epoch 61/(100,), Train Loss: 11.558666, Val Loss: 11.959120\n",
      "Epoch 62/(100,), Train Loss: 12.107155, Val Loss: 15.142788\n",
      "Epoch 63/(100,), Train Loss: 10.581300, Val Loss: 9.991925\n",
      "Epoch 64/(100,), Train Loss: 11.278292, Val Loss: 8.691875\n",
      "Epoch 65/(100,), Train Loss: 10.471950, Val Loss: 10.359777\n",
      "Epoch 66/(100,), Train Loss: 10.656971, Val Loss: 11.045614\n",
      "Epoch 67/(100,), Train Loss: 11.150022, Val Loss: 10.265978\n",
      "Epoch 68/(100,), Train Loss: 10.848660, Val Loss: 12.381875\n",
      "Epoch 69/(100,), Train Loss: 10.442062, Val Loss: 11.831015\n",
      "Epoch 70/(100,), Train Loss: 10.201911, Val Loss: 10.147750\n",
      "Epoch 71/(100,), Train Loss: 10.173689, Val Loss: 9.649683\n",
      "Epoch 72/(100,), Train Loss: 9.678850, Val Loss: 19.015854\n",
      "Epoch 73/(100,), Train Loss: 9.904502, Val Loss: 13.672583\n",
      "Epoch 74/(100,), Train Loss: 9.453226, Val Loss: 7.678987\n",
      "Epoch 75/(100,), Train Loss: 9.521131, Val Loss: 10.091869\n",
      "Epoch 76/(100,), Train Loss: 9.326722, Val Loss: 7.957144\n",
      "Epoch 77/(100,), Train Loss: 9.364224, Val Loss: 9.192683\n",
      "Epoch 78/(100,), Train Loss: 9.203431, Val Loss: 10.280570\n",
      "Epoch 79/(100,), Train Loss: 8.931161, Val Loss: 7.770257\n",
      "Epoch 80/(100,), Train Loss: 8.405233, Val Loss: 10.697140\n",
      "Epoch 81/(100,), Train Loss: 8.672059, Val Loss: 8.524368\n",
      "Epoch 82/(100,), Train Loss: 9.135340, Val Loss: 11.683901\n",
      "Epoch 83/(100,), Train Loss: 8.348395, Val Loss: 8.865997\n",
      "Epoch 84/(100,), Train Loss: 8.203150, Val Loss: 7.873653\n",
      "Epoch 85/(100,), Train Loss: 8.710810, Val Loss: 10.814927\n",
      "Epoch 86/(100,), Train Loss: 8.479743, Val Loss: 7.467157\n",
      "Epoch 87/(100,), Train Loss: 7.679787, Val Loss: 10.698512\n",
      "Epoch 88/(100,), Train Loss: 7.485958, Val Loss: 7.165753\n",
      "Epoch 89/(100,), Train Loss: 7.699055, Val Loss: 6.581071\n",
      "Epoch 90/(100,), Train Loss: 7.535417, Val Loss: 10.792740\n",
      "Epoch 91/(100,), Train Loss: 7.723052, Val Loss: 7.016041\n",
      "Epoch 92/(100,), Train Loss: 7.282448, Val Loss: 6.574333\n",
      "Epoch 93/(100,), Train Loss: 7.503121, Val Loss: 6.073766\n",
      "Epoch 94/(100,), Train Loss: 7.113707, Val Loss: 9.478080\n",
      "Epoch 95/(100,), Train Loss: 6.748617, Val Loss: 7.630074\n",
      "Epoch 96/(100,), Train Loss: 6.483081, Val Loss: 8.737521\n",
      "Epoch 97/(100,), Train Loss: 6.752267, Val Loss: 9.321286\n",
      "Epoch 98/(100,), Train Loss: 6.481025, Val Loss: 5.105740\n",
      "Epoch 99/(100,), Train Loss: 6.649195, Val Loss: 12.443727\n",
      "Epoch 100/(100,), Train Loss: 6.050651, Val Loss: 6.846799\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader,SubsetRandomSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch.optim as optim\n",
    "dataset = CustomDataset(path_file=\"data/Titanic-Dataset.csv\",numeric_columns=[\"Age\"],string_columns=[\"Name\",\"Ticket\",\"Embarked\",\"Cabin\"]\n",
    "                        ,binary_columns=[\"Sex\"]\n",
    "                        ,target_column=\"Survived\")\n",
    "\n",
    "\n",
    "\n",
    "batch_size: int = 32,\n",
    "num_epochs: int = 100,\n",
    "learning_rate = 0.001,\n",
    "l1_lambda: float = 0.01,\n",
    "patience: int = 5,\n",
    "validation_split: float = 0.2\n",
    "lr = 0.001\n",
    "\n",
    "in_features = len(dataset.df.columns) - 1  # Все колонки, кроме целевой\n",
    "out_features = 1 \n",
    "\n",
    "\n",
    "model = LinearModel(in_features, out_features)\n",
    "criterion = nn.BCEWithLogitsLoss()  # Функция потерь для регрессии\n",
    "optimizer = torch.optim.Adam(params =model.parameters(), lr=lr)\n",
    "\n",
    "    # Разделяем данные на обучающую и валидационную выборки\n",
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "train_indices, val_indices = train_test_split(\n",
    "        indices, test_size=validation_split, random_state=42\n",
    "    )\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "val_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=32, sampler=train_sampler)\n",
    "val_loader = DataLoader(dataset, batch_size=32, sampler=val_sampler)\n",
    "\n",
    "    # Инициализируем раннюю остановку\n",
    "early_stopping = EarlyStopping(patience=patience, delta=0)\n",
    "\n",
    "    # Обучение\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(100):\n",
    "        # Обучающий режим\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to( dtype=torch.float32), target.to(dtype=torch.float32)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target.view(-1, 1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "\n",
    "        # Валидация\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for data, target in val_loader:\n",
    "                data, target = data.to( dtype=torch.float32), target.to( dtype=torch.float32)\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target.view(-1, 1))  # Только MSE для валидации\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.6f}, Val Loss: {avg_val_loss:.6f}\")\n",
    "\n",
    "       \n",
    "\n",
    "   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
